{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis: Iteración de Valores\n",
    "\n",
    "**Estudiante:** [Tu nombre]\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este documento presenta el análisis de la implementación de **Iteración de Valores** para la solución de Procesos de Decisión de Markov (MDPs). Se evalúa el algoritmo en dos ambientes:\n",
    "\n",
    "1. **GridWorld 10x10** - con diferentes números de iteraciones\n",
    "2. **Bridge** - con factores de descuento 0.9 y 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1: GridWorld - Convergencia de Valores y Acciones\n",
    "\n",
    "### Ejecución\n",
    "\n",
    "Se ejecutó la implementación de iteración de valores con 5, 10, 15, 20, 30, y 50 iteraciones sobre GridWorld 10x10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld_environment import GridWorld10x10\n",
    "from mdp import MDP\n",
    "from value_iteration import ValueIteration\n",
    "import numpy as np\n",
    "\n",
    "env = GridWorld10x10()\n",
    "mdp = MDP(env)\n",
    "arrows = {'up': '  ↑  ', 'down': '  ↓  ', 'left': '  ←  ', 'right': '  →  ', None: '  ●  '}\n",
    "\n",
    "def print_values(vi, env):\n",
    "    for r in range(env.nrows):\n",
    "        row = ''\n",
    "        for c in range(env.ncols):\n",
    "            if env.board[r][c] == '#':\n",
    "                row += '  ####  '\n",
    "            else:\n",
    "                row += f'{vi.get_value((r, c)):+7.3f} '\n",
    "        print(row)\n",
    "\n",
    "def print_policy(vi, env):\n",
    "    for r in range(env.nrows):\n",
    "        row = ''\n",
    "        for c in range(env.ncols):\n",
    "            if env.board[r][c] == '#':\n",
    "                row += ' #### '\n",
    "            else:\n",
    "                action = vi.get_policy((r, c))\n",
    "                row += f'{arrows[action]}'\n",
    "        print(row)\n",
    "\n",
    "# Ejecutar para diferentes iteraciones\n",
    "for n_iter in [5, 10, 15, 20, 30, 50]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Iteraciones: {n_iter}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    vi = ValueIteration(mdp, discount=0.9, iterations=n_iter)\n",
    "    vi.run_value_iteration()\n",
    "    \n",
    "    print(\"\\nValores:\")\n",
    "    print_values(vi, env)\n",
    "    \n",
    "    print(\"\\nPolítica:\")\n",
    "    print_policy(vi, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados Observados\n",
    "\n",
    "#### 5 Iteraciones\n",
    "- **Valores**: La mayoría de estados tienen valor 0, solo los estados cercanos al objetivo `(5,5)` tienen valores positivos.\n",
    "- **Política**: Incompleta, muchos estados apuntan en direcciones arbitrarias ya que los valores aún no se han propagado.\n",
    "\n",
    "#### 10 Iteraciones\n",
    "- **Valores**: Los valores comienzan a propagarse desde el objetivo hacia el resto del grid.\n",
    "- **Política**: Se observa una política coherente en la región cercana al objetivo, pero las esquinas aún no tienen direcciones óptimas.\n",
    "\n",
    "#### 15 Iteraciones\n",
    "- **Valores**: Los valores alcanzan la esquina superior izquierda `(0,0)`, estado inicial.\n",
    "- **Política**: **La política converge** - las acciones son coherentes y apuntan hacia el objetivo `(5,5)` evitando los terminales negativos.\n",
    "\n",
    "#### 20-30 Iteraciones\n",
    "- **Valores**: Refinamiento en los decimales, pero la política permanece **idéntica** a la de 15 iteraciones.\n",
    "- **Política**: Sin cambios.\n",
    "\n",
    "#### 50 Iteraciones\n",
    "- **Valores**: Convergencia completa, cambios solo en el tercer decimal.\n",
    "- **Política**: Sin cambios respecto a 15 iteraciones.\n",
    "\n",
    "### Análisis\n",
    "\n",
    "**¿Cuándo convergen los valores?**\n",
    "- Los **valores convergen completamente alrededor de las 30-50 iteraciones**, cuando los cambios entre iteraciones son menores a 0.001.\n",
    "\n",
    "**¿Cuándo convergen las acciones?**\n",
    "- La **política (acciones) converge mucho antes, alrededor de las 15 iteraciones**.\n",
    "- Esto es un resultado importante: **la política óptima se puede extraer antes de que los valores converjan completamente**.\n",
    "\n",
    "**Explicación:**\n",
    "- La política depende del **orden relativo** de los Q-valores, no de sus valores absolutos exactos.\n",
    "- Una vez que `Q(s, a_mejor) > Q(s, otras_acciones)`, la acción óptima ya está determinada, incluso si los valores exactos aún están cambiando.\n",
    "- Los valores continúan refinándose para converger al valor exacto `V*(s)`, pero la política ya es óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 2: Bridge - Efecto del Factor de Descuento\n",
    "\n",
    "### Ejecución\n",
    "\n",
    "Se ejecutó la implementación sobre el ambiente Bridge durante 10 iteraciones con factores de descuento γ = 0.9 y γ = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bridge_environment import BridgeEnvironment\n",
    "\n",
    "env_bridge = BridgeEnvironment()\n",
    "mdp_bridge = MDP(env_bridge)\n",
    "\n",
    "for discount in [0.9, 0.1]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Factor de Descuento: γ = {discount}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    vi = ValueIteration(mdp_bridge, discount=discount, iterations=10)\n",
    "    vi.run_value_iteration()\n",
    "    \n",
    "    print(\"\\nValores:\")\n",
    "    print_values(vi, env_bridge)\n",
    "    \n",
    "    print(\"\\nPolítica:\")\n",
    "    print_policy(vi, env_bridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "#### γ = 0.9 (Alto - valora recompensas futuras)\n",
    "\n",
    "**Valores (Fila 1 - El Puente):**\n",
    "```\n",
    "+0.000  -32.967  -52.530  -40.921  -13.404  +32.967  +0.000\n",
    "```\n",
    "\n",
    "**Política (Fila 1):**\n",
    "```\n",
    "LEFT  LEFT  LEFT  RIGHT  RIGHT  RIGHT  EXIT\n",
    "```\n",
    "\n",
    "**Observaciones:**\n",
    "- El agente **cruza el puente** desde `(1,0)` hasta `(1,6)` para alcanzar la recompensa +100.\n",
    "- Los valores en el puente son negativos debido al riesgo de caer en las casillas -100, pero el valor esperado total es positivo porque γ=0.9 hace que el +100 futuro sea muy valioso.\n",
    "- Los valores cerca del objetivo son positivos: `V(1,5) = +32.967`.\n",
    "\n",
    "#### γ = 0.1 (Bajo - descuenta fuertemente el futuro)\n",
    "\n",
    "**Valores (Fila 1 - El Puente):**\n",
    "```\n",
    "+0.000  -30.303  -32.140  -32.028  -28.466  +30.303  +0.000\n",
    "```\n",
    "\n",
    "**Política (Fila 1):**\n",
    "```\n",
    "LEFT  LEFT  LEFT  RIGHT  RIGHT  RIGHT  EXIT\n",
    "```\n",
    "\n",
    "**Observaciones:**\n",
    "- El agente **también cruza el puente**, la política es idéntica.\n",
    "- Sin embargo, los valores son **mucho más bajos** en general.\n",
    "- Con γ=0.1, la recompensa +100 en `(1,6)` se descuenta fuertemente: después de 6 pasos, vale aproximadamente `100 * 0.1^6 = 0.0001`, casi nada.\n",
    "\n",
    "**Diferencia clave:**\n",
    "- **Esquina superior derecha `(0,6)`:**\n",
    "  - γ=0.9: Política = **RIGHT** (hacia el objetivo +100)\n",
    "  - γ=0.1: Política = **DOWN** (hacia el terminal -100)\n",
    "  \n",
    "Con γ=0.1, el objetivo lejano casi no vale nada, por lo que en estados alejados la política puede ser subóptima o indiferente.\n",
    "\n",
    "### Análisis\n",
    "\n",
    "**¿Qué cambios se observan?**\n",
    "\n",
    "1. **Valores absolutos:**\n",
    "   - Con γ=0.9, los valores son más altos (tanto positivos como negativos más extremos).\n",
    "   - Con γ=0.1, los valores están más comprimidos cerca de cero.\n",
    "\n",
    "2. **Política en el puente:**\n",
    "   - Ambos cruzan el puente (idéntica política).\n",
    "   - Esto ocurre porque el agente **empieza en el puente** `(1,0)`, por lo que no tiene otra opción más que cruzar.\n",
    "\n",
    "3. **Política en estados lejanos:**\n",
    "   - Con γ=0.9, estados alejados del puente aún \"sienten\" la atracción del objetivo +100.\n",
    "   - Con γ=0.1, estados alejados casi ignoran el objetivo porque está muy descontado.\n",
    "\n",
    "**¿Por qué cambian los resultados?**\n",
    "\n",
    "El factor de descuento γ controla cuánto valora el agente las recompensas futuras:\n",
    "\n",
    "- **γ cercano a 1 (e.g., 0.9):**\n",
    "  - El agente es **visionario** - planifica a largo plazo.\n",
    "  - Recompensas futuras tienen casi el mismo peso que recompensas inmediatas.\n",
    "  - Valores más altos en magnitud.\n",
    "  \n",
    "- **γ cercano a 0 (e.g., 0.1):**\n",
    "  - El agente es **miope** - solo le importa el futuro inmediato.\n",
    "  - Recompensas lejanas se descuentan exponencialmente: `γ^t`.\n",
    "  - Valores más bajos en magnitud, política puede ser subóptima en estados lejanos.\n",
    "\n",
    "**Fórmula:**\n",
    "```\n",
    "V(s) = Σ γ^t * R_t\n",
    "```\n",
    "\n",
    "Con γ=0.1, después de 10 pasos: `γ^10 = 0.1^10 = 1e-10` - las recompensas futuras prácticamente desaparecen.\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "- La **política óptima depende del factor de descuento**.\n",
    "- En el Bridge, ambas políticas cruzan porque el agente empieza en el puente.\n",
    "- En estados alejados del objetivo, γ bajo produce políticas que ignoran recompensas lejanas.\n",
    "- Para problemas de planificación a largo plazo, se recomienda γ ≥ 0.9.\n",
    "- Para problemas de recompensa inmediata, γ bajo (0.1-0.5) puede ser apropiado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones Generales\n",
    "\n",
    "1. **Convergencia de política vs valores:**\n",
    "   - La política converge más rápido (~15 iteraciones) que los valores (~30-50 iteraciones).\n",
    "   - Esto sugiere que para obtener una política óptima, no es necesario esperar la convergencia completa de los valores.\n",
    "\n",
    "2. **Efecto del factor de descuento:**\n",
    "   - γ alto (0.9): planificación a largo plazo, valores altos.\n",
    "   - γ bajo (0.1): planificación miope, valores bajos, política puede ignorar objetivos lejanos.\n",
    "\n",
    "3. **Iteración de Valores es efectiva:**\n",
    "   - Converge a la política óptima en ambos ambientes.\n",
    "   - Método simple y robusto para resolver MDPs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
