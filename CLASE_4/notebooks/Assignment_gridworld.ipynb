{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209a8ecf",
   "metadata": {},
   "source": [
    "# TOMAS ACOSTA BERNAL \n",
    "202011237"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0a6b89-5f5b-4706-8da8-c8d00ff69870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gridworld y su soluci√≥n como MDPs\n",
    "\n",
    "En este trabajo definiremos el ambiente de Gridworld y su soluci√≥n como un MDP.\n",
    "Gridworld es un ambiente cl√°sico de prueba dentro del aprendizaje por refuerzo. Durante este taller definiremos el modelo b√°sico del ambiente, que extenderemos incrementalmente de acuerdo a las necesidades del algoritmo de soluci√≥n.\n",
    "\n",
    "## Ambiente üåé\n",
    "\n",
    "El ambiente de gridworld se define como una cuadricula de `nxm`. El ambiente tiene obstaculos, es decir casillas por las cuales no puede pasar el agente. Al chocar con un obstaculo, el agente terminar√≠a en el mismo estado inicial. Adem√°s, el ambiente tiene una casilla de inicio, y algunas casillas de salida. Un ejemplo del ambiente para el caso `3x4` se muestra a continuaci√≥n.\n",
    "\n",
    "![gridworld.png](./img/gridworld.png)\n",
    "\n",
    "En este ejemplo del ambiente el agente comienza en la casilla inferior izquierda y tiene como objetivo llegar a la casilla de salida verde, con recompensa 1. La otra casilla de salida, tiene recompensa -1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21b646",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1.\n",
    "#### ¬øC√≥mo podemos codificar el ambiente?\n",
    "\n",
    "De una definici√≥n completa del ambiente, como una clase de python llamada `Environment`, estableciendo:\n",
    "1. Un atributo que define la cuadr√≠cula (`board`). El ambiente recibir√° una matriz como par√°metro describiendo la cuadr√≠cula en el momento de su creaci√≥n. Definiremos las casillas por las que puede pasar el agente como casillas vacias, las casillas por las que no puede pasar el agente con un valor none `#` y las casillas de salida con el valor asociado a la recompensa definidas para cada una de ellas.\n",
    "2. Un atributo `nrows` para almacenar la cantidad de filas de la cuadr√≠cula.\n",
    "3. Un atributo `ncols` para almacenar la cantidad de columnas de la cuadr√≠cula.\n",
    "4. Un atributo `initial_state` para almacenar el estado inicial del agente dentro del ambiente.\n",
    "5. Un atributo con el estado actual (`current_state`) en el que se encuentra el agente. El valor de `current_state` se definir√° como una tupla \n",
    "6. Un atributo `P` que guarda la matriz de probabilidades de cada una de las acciones para cada estado. Dicha matriz esta definida por par√°metro.\n",
    "\n",
    "Un ejemplo de la definici√≥n del tablero para el caso de 5x5 de la figura anterior se da a continuaci√≥n.\n",
    "```\n",
    "board = [['', ' ', ' ',  '+1'],\n",
    "         [' ', '#', ' ',  '-1'],\n",
    "         ['S', ' ', ' ', ' ']]\n",
    "```\n",
    "En el ejemplo `S` denota el estado inicial y `'#'` la casilla prohibida (manejaremos esta convenci√≥n para todos los ambientes de gridworld).\n",
    "\n",
    "De forma similar a la definici√≥n del `board` la matriz de probabilidades `P` se define como:\n",
    "```\n",
    "P = [[[0.1, 0.1, 0, 0.8], [0.1, 0.1, 0, 0.8], [0.1, 0.1, 0, 0.8],  [1]],\n",
    "         [[0.8, 0, 0.1, 0.1], '#', [0.8, 0, 0.1, 0.1],  [-1]],\n",
    "         [[0.8, 0, 0.1, 0.1], [0.1, 0.1, 0.8, 0], [0.1, 0.1, 0.8, 0], [0.1, 0.1, 0.8, 0]]]\n",
    "```\n",
    "Para la definici√≥n de `P` vamos a entender cada una de las posiciones de la probabilidad en el orden (`'up', 'down', 'left', 'right'`). Adicionalmente, vamos a suponer que la casilla da la probabilidad de tal forma que el agente siempre toma la acci√≥n en direcci√≥n al objetivo (la acci√≥n que tiene probabilidad `0.8`). Por ejemplo, para la casilla superior izquierda la probabilidad de tomar la acci√≥n `up` y llegar a la casilla de arriba es de `0.1`, a la casilla de abajo con probabilidad `0.1` y a la casilla de la derecha con  probabilidad `0.8` (el agente nunca puede llegar a la casilla de la izquierda). En las casillas de salida, el agente solo tiene una posibilidad que es tomar la acci√≥n `exit` que le da la recompensa asociada a la casilla al agente.\n",
    "\n",
    "\n",
    "#### Comportamiento del ambeinte\n",
    "\n",
    "Una vez definido el ambiente definimos su comportamiento. Para ello requerimos los siguientes m√©todos:\n",
    "1. `get_current_state` que no recibe par√°metros y retorna el estado actual (la casilla donde se encuentra el agente)\n",
    "2. `get_posible_actions` que recibe el estado actual del agente como par√°metro y retorna las acciones disponibles para dicho estado. Las acciones estar√°n dadas por su nombre (`'up', 'down', 'left', 'right'`) para las casillas normales y (`'exit'`) para las casillas de salida. Como convenci√≥n definiremos que el agente siempre puede moverse en todas las direcciones, donde un movimiento en direcci√≥n de un obst√°culo o los l√≠mites del ambiente no tienen ning√∫n efecto visible en la posici√≥n del agente.\n",
    "3. `do_action` que recibe como par√°metro la acci√≥n a ejecutar y retorna el valor de la recompensa y el nuevo estado del agente, como un pareja `reward, new_state`. Note que `do_action` esta restringida por la matriz de probabilidad `P` para la ejecuci√≥n real de las acciones.\n",
    "4. `reset` que no recibe par√°metros y restablece el ambiente a su estado inicial.\n",
    "5. `is_terminal` que no recibe par√°metros y determina si el agente est√° en el estado final o no. En nuestro caso, el estado final estar√° determinado por las casillas de salida (i.e., con un valor definido).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d612611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 20:13:17.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mInicializando ambiente de Gridworld\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.017\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mDimensiones del tablero: 3x4\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.017\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m18\u001b[0m - \u001b[34m\u001b[1mEstado inicial: (2, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.018\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[32m\u001b[1mAmbiente inicializado correctamente\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m129\u001b[0m - \u001b[1m=== Prueba de ambiente ===\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mEstado actual: (2, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.020\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_possible_actions\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mEstado (2, 0) tiene acciones: ['up', 'down', 'left', 'right']\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mAcciones posibles: ['up', 'down', 'left', 'right']\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1m\n",
      "--- Step 1 ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (2, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.056\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: acci√≥n solicitada 'right', acci√≥n ejecutada 'up'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mNuevo estado: (1, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mAcci√≥n tomada: right, Recompensa: 0, Nuevo estado: (1, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1m\n",
      "--- Step 2 ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (1, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.059\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: acci√≥n solicitada 'left', acci√≥n ejecutada 'up'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mNuevo estado: (0, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mAcci√≥n tomada: left, Recompensa: 0, Nuevo estado: (0, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1m\n",
      "--- Step 3 ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (0, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mNuevo estado: (0, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mAcci√≥n tomada: right, Recompensa: 0, Nuevo estado: (0, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1m\n",
      "--- Step 4 ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (0, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.066\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: acci√≥n solicitada 'up', acci√≥n ejecutada 'right'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mNuevo estado: (0, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mAcci√≥n tomada: up, Recompensa: 0, Nuevo estado: (0, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1m\n",
      "--- Step 5 ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (0, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.071\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: acci√≥n solicitada 'up', acci√≥n ejecutada 'right'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mEstado terminal alcanzado (0, 3) con recompensa 1\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mNuevo estado: (0, 3), Recompensa: 1\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mAcci√≥n tomada: up, Recompensa: 1, Nuevo estado: (0, 3)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.074\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m140\u001b[0m - \u001b[32m\u001b[1mEstado terminal alcanzado!\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreset\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mReseteando ambiente al estado inicial\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.076\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreset\u001b[0m:\u001b[36m105\u001b[0m - \u001b[32m\u001b[1mAmbiente reseteado. Estado actual: (2, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:13:17.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mDespu√©s del reset, estado actual: (2, 0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Union\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, board: List[List[Union[str, int, float]]], P: List[List[Union[str, List[float]]]], initial_state: Tuple[int, int]):\n",
    "        logger.info(\"Inicializando ambiente de Gridworld\")\n",
    "        \n",
    "        self.board = board\n",
    "        self.nrows = len(board)\n",
    "        self.ncols = len(board[0])\n",
    "        self.initial_state = initial_state\n",
    "        self.current_state = initial_state\n",
    "        self.P = P\n",
    "        \n",
    "        logger.debug(f\"Dimensiones del tablero: {self.nrows}x{self.ncols}\")\n",
    "        logger.debug(f\"Estado inicial: {self.initial_state}\")\n",
    "        logger.success(\"Ambiente inicializado correctamente\")\n",
    "    \n",
    "    def get_current_state(self) -> Tuple[int, int]:\n",
    "        logger.trace(f\"Obteniendo estado actual: {self.current_state}\")\n",
    "        return self.current_state\n",
    "    \n",
    "    def get_possible_actions(self, state: Tuple[int, int]) -> List[str]:\n",
    "        r, c = state\n",
    "        \n",
    "        logger.trace(f\"Obteniendo acciones posibles para estado {state}\")\n",
    "        \n",
    "        if isinstance(self.board[r][c], (int, float)):\n",
    "            logger.debug(f\"Estado {state} es terminal, acci√≥n disponible: ['exit']\")\n",
    "            return ['exit']\n",
    "        \n",
    "        actions = ['up', 'down', 'left', 'right']\n",
    "        logger.debug(f\"Estado {state} tiene acciones: {actions}\")\n",
    "        return actions\n",
    "    \n",
    "    def do_action(self, action: str) -> Tuple[Union[int, float], Tuple[int, int]]:\n",
    "        r, c = self.current_state\n",
    "        \n",
    "        logger.info(f\"Ejecutando acci√≥n '{action}' desde estado {self.current_state}\")\n",
    "        \n",
    "        if self.is_terminal():\n",
    "            if action == 'exit':\n",
    "                reward = self.board[r][c]\n",
    "                logger.success(f\"Acci√≥n 'exit' ejecutada. Recompensa: {reward}\")\n",
    "                return reward, self.current_state\n",
    "            else:\n",
    "                logger.warning(f\"Acci√≥n '{action}' inv√°lida en estado terminal. Sin efecto.\")\n",
    "                return 0, self.current_state\n",
    "        \n",
    "        if self.P[r][c] == '#':\n",
    "            logger.warning(f\"Estado {self.current_state} es un obst√°culo. Sin efecto.\")\n",
    "            return 0, self.current_state\n",
    "        \n",
    "        probs = self.P[r][c]\n",
    "        \n",
    "        action_map = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
    "        actual_action_idx = np.random.choice(4, p=probs)\n",
    "        actual_actions = ['up', 'down', 'left', 'right']\n",
    "        actual_action = actual_actions[actual_action_idx]\n",
    "        \n",
    "        if actual_action != action:\n",
    "            logger.debug(f\"Estocasticidad: acci√≥n solicitada '{action}', acci√≥n ejecutada '{actual_action}'\")\n",
    "        \n",
    "        new_state = self._calculate_new_state(r, c, actual_action)\n",
    "        \n",
    "        reward = 0\n",
    "        if isinstance(self.board[new_state[0]][new_state[1]], (int, float)):\n",
    "            reward = self.board[new_state[0]][new_state[1]]\n",
    "            logger.info(f\"Estado terminal alcanzado {new_state} con recompensa {reward}\")\n",
    "        \n",
    "        self.current_state = new_state\n",
    "        logger.info(f\"Nuevo estado: {self.current_state}, Recompensa: {reward}\")\n",
    "        \n",
    "        return reward, new_state\n",
    "    \n",
    "    def _calculate_new_state(self, r: int, c: int, action: str) -> Tuple[int, int]:\n",
    "        logger.trace(f\"Calculando nuevo estado desde ({r}, {c}) con acci√≥n '{action}'\")\n",
    "        \n",
    "        if action == 'up':\n",
    "            new_r, new_c = r - 1, c\n",
    "        elif action == 'down':\n",
    "            new_r, new_c = r + 1, c\n",
    "        elif action == 'left':\n",
    "            new_r, new_c = r, c - 1\n",
    "        elif action == 'right':\n",
    "            new_r, new_c = r, c + 1\n",
    "        else:\n",
    "            logger.error(f\"Acci√≥n desconocida: '{action}'\")\n",
    "            return (r, c)\n",
    "        \n",
    "        if (new_r < 0 or new_r >= self.nrows or \n",
    "            new_c < 0 or new_c >= self.ncols or\n",
    "            self.board[new_r][new_c] == '#'):\n",
    "            logger.debug(f\"Movimiento bloqueado (l√≠mite/obst√°culo). Permanece en ({r}, {c})\")\n",
    "            return (r, c)\n",
    "        \n",
    "        logger.trace(f\"Nuevo estado calculado: ({new_r}, {new_c})\")\n",
    "        return (new_r, new_c)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        logger.info(\"Reseteando ambiente al estado inicial\")\n",
    "        self.current_state = self.initial_state\n",
    "        logger.success(f\"Ambiente reseteado. Estado actual: {self.current_state}\")\n",
    "    \n",
    "    def is_terminal(self) -> bool:\n",
    "        r, c = self.current_state\n",
    "        is_term = isinstance(self.board[r][c], (int, float))\n",
    "        logger.trace(f\"Verificando si estado {self.current_state} es terminal: {is_term}\")\n",
    "        return is_term\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.add(\"gridworld.log\", rotation=\"500 MB\", level=\"DEBUG\")\n",
    "    \n",
    "    board = [['', ' ', ' ', 1],\n",
    "             [' ', '#', ' ', -1],\n",
    "             ['S', ' ', ' ', ' ']]\n",
    "    \n",
    "    P = [[[0.1, 0.1, 0, 0.8], [0.1, 0.1, 0, 0.8], [0.1, 0.1, 0, 0.8], [1, 0, 0, 0]],\n",
    "         [[0.8, 0, 0.1, 0.1], '#', [0.8, 0, 0.1, 0.1], [0, 1, 0, 0]],\n",
    "         [[0.8, 0, 0.1, 0.1], [0.1, 0.1, 0.8, 0], [0.1, 0.1, 0.8, 0], [0.1, 0.1, 0.8, 0]]]\n",
    "    \n",
    "    initial_state = (2, 0)\n",
    "    \n",
    "    env = Environment(board, P, initial_state)\n",
    "    \n",
    "    logger.info(\"=== Prueba de ambiente ===\")\n",
    "    logger.info(f\"Estado actual: {env.get_current_state()}\")\n",
    "    logger.info(f\"Acciones posibles: {env.get_possible_actions(env.get_current_state())}\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        logger.info(f\"\\n--- Step {i+1} ---\")\n",
    "        action = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "        reward, new_state = env.do_action(action)\n",
    "        logger.info(f\"Acci√≥n tomada: {action}, Recompensa: {reward}, Nuevo estado: {new_state}\")\n",
    "        \n",
    "        if env.is_terminal():\n",
    "            logger.success(\"Estado terminal alcanzado!\")\n",
    "            break\n",
    "    \n",
    "    env.reset()\n",
    "    logger.info(f\"Despu√©s del reset, estado actual: {env.get_current_state()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598cc01e-d7a7-4cb5-b72b-2d2adf013c9b",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la definici√≥n del agente, genere un ambiente de `10x10` como se muestra a continuaci√≥n.\n",
    "\n",
    "![evaluacion.png](./img/evaluacion.png)\n",
    "\n",
    "### Task 2.\n",
    "Plantee el problema de MDP para cada una de las casillas. Especifique el estado de inicio, las transiciones y su probabilidad (suponiendo que todas las acciones sucede con probabilidad de 0.25) y los estados de fin con su recompensa.\n",
    "¬øC√≥mo ser√≠an las recompensas esperadas para cada estado?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432feb3",
   "metadata": {},
   "source": [
    "# Task 2: Planteamiento del MDP para el ambiente 10x10\n",
    "\n",
    "## Definici√≥n del ambiente\n",
    "\n",
    "\n",
    "- **Estado inicial**: Casilla superior izquierda (0, 0) \n",
    "- **Obst√°culos**: Casillas grises que forman una barrera vertical y horizontal\n",
    "- **Estados terminales**:\n",
    "  - (4, 5): Recompensa R = -1\n",
    "  - (5, 5): Recompensa R = 1\n",
    "  - (7, 4): Recompensa R = -1\n",
    "  - (7, 5): Recompensa R = -1\n",
    "\n",
    "## Componentes del MDP\n",
    "\n",
    "### 1. S\n",
    "\n",
    "El conjunto de estados est√° compuesto por todas las casillas del grid 10√ó10, excluyendo los obst√°culos:\n",
    "```\n",
    "S = {(i, j) | 0 ‚â§ i < 10, 0 ‚â§ j < 10, (i, j) ‚àâ Obst√°culos}\n",
    "```\n",
    "\n",
    "Donde los **Obst√°culos** son:\n",
    "```\n",
    "Obst√°culos = {\n",
    "    (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7),\n",
    "    (3, 4),\n",
    "    (4, 4),\n",
    "    (5, 4),\n",
    "    (6, 4),\n",
    "    (7, 4) - estado terminal,\n",
    "    (8, 4)\n",
    "}\n",
    "```\n",
    "\n",
    "Y los **Estados Terminales** son:\n",
    "```\n",
    "S_terminal = {(4, 5), (5, 5), (7, 4), (7, 5)}\n",
    "```\n",
    "\n",
    "### 2. A\n",
    "\n",
    "Para cada estado no terminal:\n",
    "```\n",
    "A(s) = {up, down, left, right}  ‚àÄs ‚àà S \\ S_terminal\n",
    "```\n",
    "\n",
    "Para estados terminales:\n",
    "```\n",
    "A(s) = {exit}  ‚àÄs ‚àà S_terminal\n",
    "```\n",
    "\n",
    "### 3. P\n",
    "\n",
    "Dado que todas las acciones ocurren con **probabilidad uniforme de 0.25**, la funci√≥n de transici√≥n se define como:\n",
    "\n",
    "Para cualquier estado no terminal `s = (r, c)`:\n",
    "```\n",
    "P(s' | s, a) = 0.25  para cada a ‚àà {up, down, left, right}\n",
    "```\n",
    "\n",
    "Donde `s'` es el estado resultante despu√©s de ejecutar la acci√≥n `a`.\n",
    "\n",
    "#### Reglas para poder hacer la transici√≥n:\n",
    "\n",
    "1. Movimiento v√°lido: Si la acci√≥n conduce a una casilla v√°lida (dentro del grid y sin obst√°culo), el agente se mueve a esa casilla.\n",
    "\n",
    "2. Colisi√≥n con obst√°culo o l√≠mite: Si la acci√≥n conduce a un obst√°culo o fuera del grid, el agente permanece en el estado actual.\n",
    "\n",
    "3. Estados terminales: Una vez alcanzado un estado terminal, el agente permanece all√≠.\n",
    "\n",
    "\n",
    "### 4. R\n",
    "\n",
    "La funci√≥n de recompensa se define como:\n",
    "```\n",
    "R(s, a, s') = {\n",
    "    1     si s' = (5, 5)\n",
    "   -1     si s' ‚àà {(4, 5), (7, 4), (7, 5)}\n",
    "    0     en caso contrario\n",
    "}\n",
    "```\n",
    "\n",
    "O de manera equivalente:\n",
    "```\n",
    "R(s) = {\n",
    "    1     si s = (5, 5)\n",
    "   -1     si s ‚àà {(4, 5), (7, 4), (7, 5)}\n",
    "    0     para todos los dem√°s estados\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Recompensas Esperadas para Cada Estado\n",
    "\n",
    "La recompensa esperada para cada estado depende de la pol√≠tica seguida. Sin embargo, podemos calcular las recompensas inmediatas esperadas asumiendo acciones equiprobables:\n",
    "\n",
    "### Estados no terminales\n",
    "\n",
    "Para cualquier estado `s` no terminal, la recompensa esperada inmediata bajo una pol√≠tica uniforme aleatoria es:\n",
    "```\n",
    "E[R | s] = Œ£ P(a) Œ£ P(s' | s, a) R(s, a, s')\n",
    "         = 0.25 √ó Œ£_{a ‚àà A} Œ£_{s'} P(s' | s, a) R(s, a, s')\n",
    "```\n",
    "\n",
    "**Ejemplo: Estado (4, 4)**\n",
    "```\n",
    "Acciones posibles: {up, down, left, right}\n",
    "\n",
    "up:    s' = (4, 4)     R = 0\n",
    "down:  s' = (4, 4)     R = 0\n",
    "left:  s' = (4, 3)     R = 0\n",
    "right: s' = (4, 5)     R = -1\n",
    "\n",
    "E[R | (4,4)] = 0.25 √ó (0 + 0 + 0 + (-1)) = -0.25\n",
    "```\n",
    "\n",
    "**Ejemplo: Estado (4, 6)**\n",
    "```\n",
    "Acciones posibles: {up, down, left, right}\n",
    "\n",
    "up:    s' = (3, 6)     R = 0\n",
    "down:  s' = (5, 6)     R = 0\n",
    "left:  s' = (4, 5)     R = -1\n",
    "right: s' = (4, 7)     R = 0\n",
    "\n",
    "E[R | (4,6)] = 0.25 √ó (0 + 0 + (-1) + 0) = -0.25\n",
    "```\n",
    "\n",
    "**Ejemplo: Estado (5, 6)**\n",
    "```\n",
    "Acciones posibles: {up, down, left, right}\n",
    "\n",
    "up:    s' = (4, 6)     R = 0\n",
    "down:  s' = (6, 6)     R = 0\n",
    "left:  s' = (5, 5)     R = 1\n",
    "right: s' = (5, 7)     R = 0\n",
    "\n",
    "E[R | (5,6)] = 0.25 √ó (0 + 0 + 1 + 0) = 0.25\n",
    "```\n",
    "\n",
    "### Estados terminales\n",
    "\n",
    "Para estados terminales, la recompensa esperada es simplemente la recompensa del estado:\n",
    "```\n",
    "E[R | (4, 5)] = -1\n",
    "E[R | (5, 5)] = 1\n",
    "E[R | (7, 4)] = -1\n",
    "E[R | (7, 5)] = -1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806898c8",
   "metadata": {},
   "source": [
    "### Task 3.\n",
    "Bajo la definci√≥n del problema anterior, suponga que cada acci√≥n tiene una probabilidad de √©xito de 60%, con probabilidad de 20% se ejecutar√° la sigiente acci√≥n (en direcci√≥n de las manesillas del reloj), con probabilidad de 10% se ejecutar√° la sigiente acci√≥n (en contra de las manesillas del reloj) y con probabilidad de 10% no pasar√° nada. Bajo estas condiciones, ¬øC√≥mo ser√≠an las recompensas esperadas para cada estado? \n",
    "\n",
    "Codifique el ambiente para el gridworld de `10x10` utilizando esta funci√≥n de probabilidad. En esta codificaci√≥n del ambiente no es necesario pasar la matriz `P` como par√°metro, pero esta informaci√≥n se debe tener en cuenta en la funci√≥n `do_action`.\n",
    "\n",
    "Tenga en cuenta que la calidad del programa que entreguen ser√° tenida en cuentra dentro de la calificaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d59764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 20:20:45.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mInicializando ambiente GridWorld 10x10\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.199\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_initialize_board\u001b[0m:\u001b[36m45\u001b[0m - \u001b[34m\u001b[1mTablero inicializado con estados terminales\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.200\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_initialize_obstacles\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mObst√°culos inicializados: 12 casillas bloqueadas\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.203\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m30\u001b[0m - \u001b[32m\u001b[1mAmbiente inicializado correctamente\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.203\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[34m\u001b[1mDimensiones: 10x10\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 20:20:45.204\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mEstado inicial: (0, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.205\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[34m\u001b[1mProbabilidades - √âxito: 0.6, Horario: 0.2, Antihorario: 0.1, Permanencia: 0.1\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1m=== Visualizaci√≥n del tablero ===\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprint_board\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mImprimiendo estado del tablero\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1m=== An√°lisis de recompensas esperadas ===\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1m\n",
      "--- Estado inicial: (0, 0) ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1m\n",
      "--- Adyacente a terminal negativo: (4, 4) ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1m\n",
      "--- Adyacente a terminal positivo: (5, 6) ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1m\n",
      "--- Adyacente a obst√°culos: (2, 0) ---\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1m\n",
      "=== Simulaci√≥n de episodio ===\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreset\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mReseteando ambiente al estado inicial\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.227\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreset\u001b[0m:\u001b[36m172\u001b[0m - \u001b[32m\u001b[1mAmbiente reseteado. Estado: (0, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (0, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 1: (0, 0) --[right]--> (0, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (0, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 2: (0, 1) --[up]--> (0, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (0, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 3: (0, 1) --[up]--> (0, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (0, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.292\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'up' ‚Üí ejecuci√≥n 'right'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 4: (0, 1) --[up]--> (0, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (0, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 3), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 5: (0, 2) --[right]--> (0, 3), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (0, 3)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 4), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 6: (0, 3) --[right]--> (0, 4), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (0, 4)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GRIDWORLD 10x10\n",
      "==================================================\n",
      "[A] .  .  .  .  .  .  .  .  . \n",
      " .  .  .  .  .  .  .  .  .  . \n",
      " .  #  #  #  #  #  #  #  .  . \n",
      " .  .  .  .  #  .  .  .  .  . \n",
      " .  .  .  .  # -1  .  .  .  . \n",
      " .  .  .  .  # +1  .  .  .  . \n",
      " .  .  .  .  #  .  .  .  .  . \n",
      " .  .  .  . -1 -1  .  .  .  . \n",
      " .  .  .  .  #  .  .  .  .  . \n",
      " .  .  .  .  .  .  .  .  .  . \n",
      "==================================================\n",
      "Estado actual: (0, 0)\n",
      "Terminal: False\n",
      "==================================================\n",
      "\n",
      "E[R | (0, 0),    up] = +0.000\n",
      "E[R | (0, 0),  down] = +0.000\n",
      "E[R | (0, 0),  left] = +0.000\n",
      "E[R | (0, 0), right] = +0.000\n",
      "E[R | (4, 4),    up] = -0.200\n",
      "E[R | (4, 4),  down] = -0.100\n",
      "E[R | (4, 4),  left] = +0.000\n",
      "E[R | (4, 4), right] = -0.600\n",
      "E[R | (5, 6),    up] = +0.100\n",
      "E[R | (5, 6),  down] = +0.200\n",
      "E[R | (5, 6),  left] = +0.600\n",
      "E[R | (5, 6), right] = +0.000\n",
      "E[R | (2, 0),    up] = +0.000\n",
      "E[R | (2, 0),  down] = +0.000\n",
      "E[R | (2, 0),  left] = +0.000\n",
      "E[R | (2, 0), right] = +0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 20:20:45.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'up' ‚Üí ejecuci√≥n 'left'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 3), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 7: (0, 4) --[up]--> (0, 3), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (0, 3)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 8: (0, 3) --[left]--> (0, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (0, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 9: (0, 2) --[up]--> (0, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (0, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m152\u001b[0m - \u001b[34m\u001b[1mAcci√≥n 'stay' ejecutada. Permanece en (0, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.416\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'right' ‚Üí ejecuci√≥n 'stay'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 10: (0, 2) --[right]--> (0, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (0, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 11: (0, 2) --[up]--> (0, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (0, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.447\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'right' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (1, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 12: (0, 2) --[right]--> (1, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (1, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (1, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 13: (1, 2) --[left]--> (1, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (1, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (1, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 14: (1, 1) --[left]--> (1, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (1, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 15: (1, 0) --[up]--> (0, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (0, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (0, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 16: (0, 0) --[left]--> (0, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (0, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.464\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (1, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 17: (0, 0) --[left]--> (1, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (1, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (2, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 18: (1, 0) --[down]--> (2, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (2, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 19: (2, 0) --[down]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.488\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m152\u001b[0m - \u001b[34m\u001b[1mAcci√≥n 'stay' ejecutada. Permanece en (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.489\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'down' ‚Üí ejecuci√≥n 'stay'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 20: (3, 0) --[down]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.495\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m152\u001b[0m - \u001b[34m\u001b[1mAcci√≥n 'stay' ejecutada. Permanece en (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.496\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'right' ‚Üí ejecuci√≥n 'stay'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 21: (3, 0) --[right]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.500\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 22: (3, 0) --[left]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 23: (4, 0) --[right]--> (4, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (4, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 24: (4, 1) --[left]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 25: (4, 0) --[down]--> (5, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m152\u001b[0m - \u001b[34m\u001b[1mAcci√≥n 'stay' ejecutada. Permanece en (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'stay'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 26: (5, 0) --[left]--> (5, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 27: (5, 0) --[up]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 28: (4, 0) --[up]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 29: (3, 0) --[down]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 30: (4, 0) --[right]--> (4, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (4, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 31: (4, 1) --[right]--> (4, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (4, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 32: (4, 2) --[down]--> (5, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (5, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 33: (5, 2) --[up]--> (4, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (4, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.554\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'right' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 34: (4, 2) --[right]--> (5, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (5, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 35: (5, 2) --[left]--> (5, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (5, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.562\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'down' ‚Üí ejecuci√≥n 'left'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 36: (5, 1) --[down]--> (5, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.567\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'up' ‚Üí ejecuci√≥n 'left'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 37: (5, 0) --[up]--> (5, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 38: (5, 0) --[up]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 39: (4, 0) --[up]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.579\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'right' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 40: (3, 0) --[right]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 41: (4, 0) --[down]--> (5, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 42: (5, 0) --[up]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 43: (4, 0) --[up]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 44: (3, 0) --[left]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 45: (3, 0) --[right]--> (3, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (3, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 46: (3, 1) --[down]--> (4, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (4, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.611\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'up' ‚Üí ejecuci√≥n 'left'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 47: (4, 1) --[up]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 48: (4, 0) --[right]--> (4, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (4, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 49: (4, 1) --[up]--> (3, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (3, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 50: (3, 1) --[down]--> (4, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (4, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'up'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 51: (4, 1) --[left]--> (3, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (3, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 52: (3, 1) --[up]--> (3, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (3, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 53: (3, 1) --[up]--> (3, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (3, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.663\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'up' ‚Üí ejecuci√≥n 'left'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 54: (3, 1) --[up]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.671\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'up'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (2, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 55: (3, 0) --[left]--> (2, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (2, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 56: (2, 0) --[down]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.688\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m152\u001b[0m - \u001b[34m\u001b[1mAcci√≥n 'stay' ejecutada. Permanece en (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.689\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'up' ‚Üí ejecuci√≥n 'stay'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 57: (3, 0) --[up]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.697\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'right' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 58: (3, 0) --[right]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 59: (4, 0) --[up]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 60: (3, 0) --[down]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 61: (4, 0) --[up]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.742\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m152\u001b[0m - \u001b[34m\u001b[1mAcci√≥n 'stay' ejecutada. Permanece en (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.744\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'stay'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 62: (3, 0) --[left]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 63: (3, 0) --[down]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 64: (4, 0) --[down]--> (5, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.757\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'down' ‚Üí ejecuci√≥n 'left'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 65: (5, 0) --[down]--> (5, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'right' ‚Üí ejecuci√≥n 'up'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 66: (5, 0) --[right]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 67: (4, 0) --[up]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (2, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 68: (3, 0) --[up]--> (2, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (2, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'right' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (3, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 69: (2, 0) --[right]--> (3, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (3, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (4, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 70: (3, 0) --[down]--> (4, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (4, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (5, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 71: (4, 0) --[left]--> (5, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (5, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (6, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 72: (5, 0) --[down]--> (6, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (6, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.854\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'up' ‚Üí ejecuci√≥n 'right'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (6, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 73: (6, 0) --[up]--> (6, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (6, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (6, 0), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 74: (6, 1) --[left]--> (6, 0), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (6, 0)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (6, 1), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 75: (6, 0) --[right]--> (6, 1), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'up' desde estado (6, 1)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.880\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'up' ‚Üí ejecuci√≥n 'right'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (6, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 76: (6, 1) --[up]--> (6, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (6, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.887\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'down'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (7, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 77: (6, 2) --[left]--> (7, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (7, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.892\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'up'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (6, 2), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 78: (7, 2) --[left]--> (6, 2), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (6, 2)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (6, 3), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 79: (6, 2) --[right]--> (6, 3), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'left' desde estado (6, 3)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.901\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m152\u001b[0m - \u001b[34m\u001b[1mAcci√≥n 'stay' ejecutada. Permanece en (6, 3)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.902\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m157\u001b[0m - \u001b[34m\u001b[1mEstocasticidad: intenci√≥n 'left' ‚Üí ejecuci√≥n 'stay'\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (6, 3), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 80: (6, 3) --[left]--> (6, 3), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'down' desde estado (6, 3)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (7, 3), Recompensa: 0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 81: (6, 3) --[down]--> (7, 3), R=+0.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEjecutando acci√≥n 'right' desde estado (7, 3)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mEstado terminal alcanzado (7, 4) con recompensa -1\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_action\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mNuevo estado: (7, 4), Recompensa: -1\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mStep 82: (7, 3) --[right]--> (7, 4), R=-1.0\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.917\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m285\u001b[0m - \u001b[32m\u001b[1m\n",
      "Episodio terminado en 82 pasos\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.919\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m286\u001b[0m - \u001b[32m\u001b[1mRecompensa total acumulada: -1.00\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.920\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m287\u001b[0m - \u001b[32m\u001b[1mEstado final: (7, 4)\u001b[0m\n",
      "\u001b[32m2026-02-16 20:20:45.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprint_board\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mImprimiendo estado del tablero\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GRIDWORLD 10x10\n",
      "==================================================\n",
      " S  .  .  .  .  .  .  .  .  . \n",
      " .  .  .  .  .  .  .  .  .  . \n",
      " .  #  #  #  #  #  #  #  .  . \n",
      " .  .  .  .  #  .  .  .  .  . \n",
      " .  .  .  .  # -1  .  .  .  . \n",
      " .  .  .  .  # +1  .  .  .  . \n",
      " .  .  .  .  #  .  .  .  .  . \n",
      " .  .  .  . [A]-1  .  .  .  . \n",
      " .  .  .  .  #  .  .  .  .  . \n",
      " .  .  .  .  .  .  .  .  .  . \n",
      "==================================================\n",
      "Estado actual: (7, 4)\n",
      "Terminal: True\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "\n",
    "class GridWorld10x10:\n",
    "    def __init__(self):\n",
    "        logger.info(\"Inicializando ambiente GridWorld 10x10\")\n",
    "        \n",
    "        self.nrows = 10\n",
    "        self.ncols = 10\n",
    "        self.initial_state = (0, 0)\n",
    "        self.current_state = self.initial_state\n",
    "        \n",
    "        self._initialize_board()\n",
    "        self._initialize_obstacles()\n",
    "        \n",
    "        self.action_success_prob = 0.60\n",
    "        self.clockwise_prob = 0.20\n",
    "        self.counterclockwise_prob = 0.10\n",
    "        self.stay_prob = 0.10\n",
    "        \n",
    "        self.action_map = {\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0),\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1)\n",
    "        }\n",
    "        \n",
    "        logger.success(\"Ambiente inicializado correctamente\")\n",
    "        logger.debug(f\"Dimensiones: {self.nrows}x{self.ncols}\")\n",
    "        logger.debug(f\"Estado inicial: {self.initial_state}\")\n",
    "        logger.debug(f\"Probabilidades - √âxito: {self.action_success_prob}, Horario: {self.clockwise_prob}, Antihorario: {self.counterclockwise_prob}, Permanencia: {self.stay_prob}\")\n",
    "    \n",
    "    def _initialize_board(self):\n",
    "        self.board = [[' ' for _ in range(self.ncols)] for _ in range(self.nrows)]\n",
    "        \n",
    "        self.board[0][0] = 'S'\n",
    "        \n",
    "        self.board[4][5] = -1\n",
    "        self.board[5][5] = 1\n",
    "        self.board[7][4] = -1\n",
    "        self.board[7][5] = -1\n",
    "        \n",
    "        logger.debug(\"Tablero inicializado con estados terminales\")\n",
    "    \n",
    "    def _initialize_obstacles(self):\n",
    "        obstacles = [\n",
    "            (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7),\n",
    "            (3, 4),\n",
    "            (4, 4),\n",
    "            (5, 4),\n",
    "            (6, 4),\n",
    "            (8, 4)\n",
    "        ]\n",
    "        \n",
    "        for r, c in obstacles:\n",
    "            self.board[r][c] = '#'\n",
    "        \n",
    "        logger.debug(f\"Obst√°culos inicializados: {len(obstacles)} casillas bloqueadas\")\n",
    "    \n",
    "    def _get_clockwise_action(self, action: str) -> str:\n",
    "        clockwise_map = {\n",
    "            'up': 'right',\n",
    "            'right': 'down',\n",
    "            'down': 'left',\n",
    "            'left': 'up'\n",
    "        }\n",
    "        return clockwise_map.get(action, action)\n",
    "    \n",
    "    def _get_counterclockwise_action(self, action: str) -> str:\n",
    "        counterclockwise_map = {\n",
    "            'up': 'left',\n",
    "            'left': 'down',\n",
    "            'down': 'right',\n",
    "            'right': 'up'\n",
    "        }\n",
    "        return counterclockwise_map.get(action, action)\n",
    "    \n",
    "    def _calculate_new_state(self, r: int, c: int, action: str) -> Tuple[int, int]:\n",
    "        if action not in self.action_map:\n",
    "            logger.warning(f\"Acci√≥n desconocida: '{action}'\")\n",
    "            return (r, c)\n",
    "        \n",
    "        dr, dc = self.action_map[action]\n",
    "        new_r, new_c = r + dr, c + dc\n",
    "        \n",
    "        if (new_r < 0 or new_r >= self.nrows or \n",
    "            new_c < 0 or new_c >= self.ncols or\n",
    "            self.board[new_r][new_c] == '#'):\n",
    "            logger.trace(f\"Movimiento bloqueado desde ({r},{c}) hacia '{action}'. Permanece en lugar\")\n",
    "            return (r, c)\n",
    "        \n",
    "        logger.trace(f\"Movimiento v√°lido desde ({r},{c}) hacia ({new_r},{new_c}) con acci√≥n '{action}'\")\n",
    "        return (new_r, new_c)\n",
    "    \n",
    "    def get_current_state(self) -> Tuple[int, int]:\n",
    "        logger.trace(f\"Estado actual: {self.current_state}\")\n",
    "        return self.current_state\n",
    "    \n",
    "    def get_possible_actions(self, state: Tuple[int, int]) -> List[str]:\n",
    "        r, c = state\n",
    "        \n",
    "        if isinstance(self.board[r][c], (int, float)):\n",
    "            logger.debug(f\"Estado {state} es terminal, acci√≥n: ['exit']\")\n",
    "            return ['exit']\n",
    "        \n",
    "        actions = ['up', 'down', 'left', 'right']\n",
    "        logger.debug(f\"Estado {state} acciones disponibles: {actions}\")\n",
    "        return actions\n",
    "    \n",
    "    def do_action(self, action: str) -> Tuple[float, Tuple[int, int]]:\n",
    "        r, c = self.current_state\n",
    "        \n",
    "        logger.info(f\"Ejecutando acci√≥n '{action}' desde estado {self.current_state}\")\n",
    "        \n",
    "        if self.is_terminal():\n",
    "            if action == 'exit':\n",
    "                reward = self.board[r][c]\n",
    "                logger.success(f\"Acci√≥n 'exit' ejecutada. Recompensa: {reward}\")\n",
    "                return reward, self.current_state\n",
    "            else:\n",
    "                logger.warning(f\"Acci√≥n '{action}' inv√°lida en estado terminal\")\n",
    "                return 0, self.current_state\n",
    "        \n",
    "        if action not in ['up', 'down', 'left', 'right']:\n",
    "            logger.error(f\"Acci√≥n desconocida: '{action}'\")\n",
    "            return 0, self.current_state\n",
    "        \n",
    "        clockwise_action = self._get_clockwise_action(action)\n",
    "        counterclockwise_action = self._get_counterclockwise_action(action)\n",
    "        \n",
    "        actions_with_probs = [\n",
    "            (action, self.action_success_prob),\n",
    "            (clockwise_action, self.clockwise_prob),\n",
    "            (counterclockwise_action, self.counterclockwise_prob),\n",
    "            ('stay', self.stay_prob)\n",
    "        ]\n",
    "        \n",
    "        rand = np.random.random()\n",
    "        cumulative_prob = 0\n",
    "        executed_action = action\n",
    "        \n",
    "        for act, prob in actions_with_probs:\n",
    "            cumulative_prob += prob\n",
    "            if rand < cumulative_prob:\n",
    "                executed_action = act\n",
    "                break\n",
    "        \n",
    "        if executed_action == 'stay':\n",
    "            new_state = self.current_state\n",
    "            logger.debug(f\"Acci√≥n 'stay' ejecutada. Permanece en {self.current_state}\")\n",
    "        else:\n",
    "            new_state = self._calculate_new_state(r, c, executed_action)\n",
    "        \n",
    "        if executed_action != action:\n",
    "            logger.debug(f\"Estocasticidad: intenci√≥n '{action}' ‚Üí ejecuci√≥n '{executed_action}'\")\n",
    "        \n",
    "        reward = 0\n",
    "        if isinstance(self.board[new_state[0]][new_state[1]], (int, float)):\n",
    "            reward = self.board[new_state[0]][new_state[1]]\n",
    "            logger.info(f\"Estado terminal alcanzado {new_state} con recompensa {reward}\")\n",
    "        \n",
    "        self.current_state = new_state\n",
    "        logger.info(f\"Nuevo estado: {self.current_state}, Recompensa: {reward}\")\n",
    "        \n",
    "        return reward, new_state\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        logger.info(\"Reseteando ambiente al estado inicial\")\n",
    "        self.current_state = self.initial_state\n",
    "        logger.success(f\"Ambiente reseteado. Estado: {self.current_state}\")\n",
    "    \n",
    "    def is_terminal(self) -> bool:\n",
    "        r, c = self.current_state\n",
    "        is_term = isinstance(self.board[r][c], (int, float))\n",
    "        logger.trace(f\"¬øEstado {self.current_state} es terminal? {is_term}\")\n",
    "        return is_term\n",
    "    \n",
    "    def get_expected_reward(self, state: Tuple[int, int], action: str) -> float:\n",
    "        r, c = state\n",
    "        \n",
    "        if isinstance(self.board[r][c], (int, float)):\n",
    "            return self.board[r][c]\n",
    "        \n",
    "        if action not in ['up', 'down', 'left', 'right']:\n",
    "            return 0.0\n",
    "        \n",
    "        clockwise_action = self._get_clockwise_action(action)\n",
    "        counterclockwise_action = self._get_counterclockwise_action(action)\n",
    "        \n",
    "        actions_with_probs = [\n",
    "            (action, self.action_success_prob),\n",
    "            (clockwise_action, self.clockwise_prob),\n",
    "            (counterclockwise_action, self.counterclockwise_prob),\n",
    "            ('stay', self.stay_prob)\n",
    "        ]\n",
    "        \n",
    "        expected_reward = 0.0\n",
    "        \n",
    "        for act, prob in actions_with_probs:\n",
    "            if act == 'stay':\n",
    "                next_state = (r, c)\n",
    "            else:\n",
    "                next_state = self._calculate_new_state(r, c, act)\n",
    "            \n",
    "            next_r, next_c = next_state\n",
    "            if isinstance(self.board[next_r][next_c], (int, float)):\n",
    "                reward = self.board[next_r][next_c]\n",
    "            else:\n",
    "                reward = 0\n",
    "            \n",
    "            expected_reward += prob * reward\n",
    "        \n",
    "        logger.trace(f\"E[R | {state}, {action}] = {expected_reward:.3f}\")\n",
    "        return expected_reward\n",
    "    \n",
    "    def print_board(self) -> None:\n",
    "        logger.info(\"Imprimiendo estado del tablero\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"GRIDWORLD 10x10\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for i in range(self.nrows):\n",
    "            row_str = \"\"\n",
    "            for j in range(self.ncols):\n",
    "                if (i, j) == self.current_state:\n",
    "                    row_str += \"[A]\"\n",
    "                elif self.board[i][j] == '#':\n",
    "                    row_str += \" # \"\n",
    "                elif self.board[i][j] == 'S':\n",
    "                    row_str += \" S \"\n",
    "                elif isinstance(self.board[i][j], (int, float)):\n",
    "                    row_str += f\"{self.board[i][j]:+2} \"\n",
    "                else:\n",
    "                    row_str += \" . \"\n",
    "            print(row_str)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(f\"Estado actual: {self.current_state}\")\n",
    "        print(f\"Terminal: {self.is_terminal()}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.add(\"gridworld_10x10.log\", level=\"DEBUG\")\n",
    "    \n",
    "    env = GridWorld10x10()\n",
    "    \n",
    "    logger.info(\"=== Visualizaci√≥n del tablero ===\")\n",
    "    env.print_board()\n",
    "    \n",
    "    logger.info(\"=== An√°lisis de recompensas esperadas ===\")\n",
    "    \n",
    "    test_states = [\n",
    "        ((0, 0), \"Estado inicial\"),\n",
    "        ((4, 4), \"Adyacente a terminal negativo\"),\n",
    "        ((5, 6), \"Adyacente a terminal positivo\"),\n",
    "        ((2, 0), \"Adyacente a obst√°culos\")\n",
    "    ]\n",
    "    \n",
    "    for state, description in test_states:\n",
    "        logger.info(f\"\\n--- {description}: {state} ---\")\n",
    "        for action in ['up', 'down', 'left', 'right']:\n",
    "            expected_r = env.get_expected_reward(state, action)\n",
    "            print(f\"E[R | {state}, {action:>5}] = {expected_r:+.3f}\")\n",
    "    \n",
    "    logger.info(\"\\n=== Simulaci√≥n de episodio ===\")\n",
    "    env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    max_steps = 1000\n",
    "    \n",
    "    while not env.is_terminal() and steps < max_steps:\n",
    "        state = env.get_current_state()\n",
    "        action = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "        \n",
    "        reward, new_state = env.do_action(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        logger.info(f\"Step {steps}: {state} --[{action}]--> {new_state}, R={reward:+.1f}\")\n",
    "    \n",
    "    logger.success(f\"\\nEpisodio terminado en {steps} pasos\")\n",
    "    logger.success(f\"Recompensa total acumulada: {total_reward:+.2f}\")\n",
    "    logger.success(f\"Estado final: {env.get_current_state()}\")\n",
    "    \n",
    "    env.print_board()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a4de2",
   "metadata": {},
   "source": [
    "\n",
    "### Task 4. \n",
    "Defina una situaci√≥n de la vide real (de su escogencia) como un MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a2aa3",
   "metadata": {},
   "source": [
    "# MDP: Optimizaci√≥n de Prompts para Agentes de Servicio al Cliente\n",
    "\n",
    "## Contexto del Problema\n",
    "\n",
    "Cuando trabajo en agentes conversacionales, constantemente necesito iterar y optimizar prompts para mejorar la calidad de las respuestas del agente. Este proceso de optimizaci√≥n puede modelarse como un MDP donde el objetivo es alcanzar un prompt de alta calidad minimizando el tiempo y costo de iteraciones.\n",
    "\n",
    "## Componentes del MDP\n",
    "\n",
    "### 1. Estados (S)\n",
    "\n",
    "El estado del sistema se representa como una tupla: `s = (quality_score, iteration_count, prompt_structure)`\n",
    "\n",
    "Donde:\n",
    "- **quality_score**: Calidad actual del prompt medida en el rango [0, 100]\n",
    "  - 0-30: Pobre (respuestas inconsistentes, errores frecuentes)\n",
    "  - 31-60: Aceptable (funciona pero necesita mejoras)\n",
    "  - 61-85: Bueno (cumple requisitos b√°sicos)\n",
    "  - 86-100: Excelente (producci√≥n-ready)\n",
    "\n",
    "- **iteration_count**: N√∫mero de iteraciones realizadas [0, 1, 2, 3, 4, 5+]\n",
    "\n",
    "- **prompt_structure**: Estado actual de la estructura del prompt\n",
    "  - `basic`: Instrucciones simples sin ejemplos\n",
    "  - `with_examples`: Incluye 2-3 ejemplos\n",
    "  - `structured`: Con formato XML/JSON y secciones claras\n",
    "  - `advanced`: Incluye chain-of-thought, pocos ejemplos, manejo de errores\n",
    "\n",
    "**Ejemplo de estados:**\n",
    "```\n",
    "s1 = (25, 0, 'basic')           # Prompt inicial pobre\n",
    "s2 = (45, 1, 'with_examples')   # Primera iteraci√≥n con ejemplos\n",
    "s3 = (72, 2, 'structured')      # Segunda iteraci√≥n estructurada\n",
    "s4 = (88, 3, 'advanced')        # Prompt optimizado\n",
    "```\n",
    "\n",
    "**Estados terminales:**\n",
    "- `quality_score >= 85`: Prompt aprobado para producci√≥n\n",
    "- `iteration_count >= 6`: L√≠mite de iteraciones alcanzado (requiere replantear estrategia)\n",
    "\n",
    "### 2. Acciones (A)\n",
    "\n",
    "Las acciones disponibles representan diferentes estrategias de optimizaci√≥n:\n",
    "```\n",
    "A = {\n",
    "    'add_examples',          # Agregar 2-3 ejemplos de few shot\n",
    "    'restructure',           # Cambiar formato (a XML, JSON, con secciones)\n",
    "    'adjust_temperature',    # Modificar temperatura\n",
    "    'add_constraints',       # Agregar reglas/restricciones espec√≠ficas\n",
    "    'add_cot',              # Agregar chain-of-thought reasoning\n",
    "    'simplify',             # Simplificar prompt \n",
    "    'test_edge_cases',      # Evaluar\n",
    "    'deploy'                # Desplegar a producci√≥n (solo si quality >= 85)\n",
    "}\n",
    "```\n",
    "\n",
    "**Restricciones por estado:**\n",
    "- Si `prompt_structure = 'basic'`: No puedes hacer `simplify`\n",
    "- Si `quality_score < 85`: No puedes hacer `deploy`\n",
    "- Si `iteration_count >= 6`: Solo puedes hacer `deploy` o reiniciar\n",
    "\n",
    "### 3. Funci√≥n de Transici√≥n P(s' | s, a)\n",
    "\n",
    "La transici√≥n es estoc√°stica porque el resultado de modificar un prompt tiene incertidumbre inherente.\n",
    "\n",
    "#### Ejemplo 1: `add_examples` desde estado `(45, 1, 'basic')`\n",
    "```\n",
    "Acci√≥n: add_examples\n",
    "Estado actual: (45, 1, 'basic')\n",
    "\n",
    "Posibles resultados:\n",
    "- (65, 2, 'with_examples'): prob = 0.60  # Mejora significativa\n",
    "- (55, 2, 'with_examples'): prob = 0.25  # Mejora moderada\n",
    "- (48, 2, 'with_examples'): prob = 0.10  # Mejora m√≠nima\n",
    "- (40, 2, 'with_examples'): prob = 0.05  # Empeora (ejemplos confusos)\n",
    "```\n",
    "\n",
    "#### Ejemplo 2: `restructure` desde estado `(55, 2, 'with_examples')`\n",
    "```\n",
    "Acci√≥n: restructure\n",
    "Estado actual: (55, 2, 'with_examples')\n",
    "\n",
    "Posibles resultados:\n",
    "- (75, 3, 'structured'): prob = 0.50  # Estructura ayuda mucho\n",
    "- (62, 3, 'structured'): prob = 0.30  # Mejora moderada\n",
    "- (50, 3, 'structured'): prob = 0.15  # Poca mejora\n",
    "- (45, 3, 'structured'): prob = 0.05  # Estructura muy compleja\n",
    "```\n",
    "\n",
    "#### Ejemplo 3: `adjust_temperature` desde estado `(72, 2, 'structured')`\n",
    "```\n",
    "Acci√≥n: adjust_temperature\n",
    "Estado actual: (72, 2, 'structured')\n",
    "\n",
    "Posibles resultados:\n",
    "- (82, 3, 'structured'): prob = 0.40  # Temperatura √≥ptima encontrada\n",
    "- (75, 3, 'structured'): prob = 0.35  # Mejora leve\n",
    "- (70, 3, 'structured'): prob = 0.20  # Sin cambio significativo\n",
    "- (65, 3, 'structured'): prob = 0.05  # Temperatura incorrecta\n",
    "```\n",
    "\n",
    "#### Ejemplo 4: `add_cot` desde estado `(75, 3, 'structured')`\n",
    "```\n",
    "Acci√≥n: add_cot\n",
    "Estado actual: (75, 3, 'structured')\n",
    "\n",
    "Posibles resultados:\n",
    "- (90, 4, 'advanced'): prob = 0.55  # CoT mejora razonamiento\n",
    "- (82, 4, 'advanced'): prob = 0.30  # Mejora moderada\n",
    "- (72, 4, 'advanced'): prob = 0.10  # Poco efecto\n",
    "- (68, 4, 'advanced'): prob = 0.05  # CoT a√±ade confusi√≥n\n",
    "```\n",
    "\n",
    "### 4. Funci√≥n de Recompensa R(s, a, s')\n",
    "\n",
    "La recompensa refleja el **trade-off entre calidad y costo/tiempo**:\n",
    "```python\n",
    "def R(s, a, s_prime):\n",
    "    quality_old, iter_old, struct_old = s\n",
    "    quality_new, iter_new, struct_new = s_prime\n",
    "    \n",
    "    # Recompensa base por mejora de calidad\n",
    "    quality_improvement = quality_new - quality_old\n",
    "    \n",
    "    # Penalizaci√≥n por iteraci√≥n (tiempo/costo)\n",
    "    iteration_penalty = -5\n",
    "    \n",
    "    # Bonificaci√≥n por alcanzar producci√≥n\n",
    "    if quality_new >= 85:\n",
    "        production_bonus = 100\n",
    "    else:\n",
    "        production_bonus = 0\n",
    "    \n",
    "    # Penalizaci√≥n fuerte por exceder iteraciones\n",
    "    if iter_new >= 6:\n",
    "        excess_penalty = -50\n",
    "    else:\n",
    "        excess_penalty = 0\n",
    "    \n",
    "    # Penalizaci√≥n por empeorar\n",
    "    if quality_improvement < 0:\n",
    "        degradation_penalty = quality_improvement * 2  # Doble penalizaci√≥n\n",
    "    else:\n",
    "        degradation_penalty = 0\n",
    "    \n",
    "    total_reward = (\n",
    "        quality_improvement + \n",
    "        iteration_penalty + \n",
    "        production_bonus + \n",
    "        excess_penalty + \n",
    "        degradation_penalty\n",
    "    )\n",
    "    \n",
    "    return total_reward\n",
    "```\n",
    "\n",
    "#### Ejemplos de Recompensas:\n",
    "\n",
    "**Caso 1: Mejora significativa**\n",
    "```\n",
    "s = (45, 1, 'basic')\n",
    "a = 'add_examples'\n",
    "s' = (65, 2, 'with_examples')\n",
    "\n",
    "R = (65-45) + (-5) + 0 + 0 + 0 = +15\n",
    "```\n",
    "\n",
    "**Caso 2: Alcanzar producci√≥n**\n",
    "```\n",
    "s = (75, 3, 'structured')\n",
    "a = 'add_cot'\n",
    "s' = (90, 4, 'advanced')\n",
    "\n",
    "R = (90-75) + (-5) + 100 + 0 + 0 = +110\n",
    "```\n",
    "\n",
    "**Caso 3: Empeorar el prompt**\n",
    "```\n",
    "s = (55, 2, 'with_examples')\n",
    "a = 'restructure'\n",
    "s' = (45, 3, 'structured')\n",
    "\n",
    "R = (45-55) + (-5) + 0 + 0 + (10*2) = -25\n",
    "```\n",
    "\n",
    "**Caso 4: Exceder iteraciones**\n",
    "```\n",
    "s = (70, 5, 'structured')\n",
    "a = 'adjust_temperature'\n",
    "s' = (72, 6, 'structured')\n",
    "\n",
    "R = (72-70) + (-5) + 0 + (-50) + 0 = -53\n",
    "```\n",
    "\n",
    "### 5. Factor de Descuento (Œ≥)\n",
    "```\n",
    "Œ≥ = 0.9\n",
    "```\n",
    "\n",
    "Un factor alto (0.9) porque queremos que el agente considere recompensas futuras y no solo mejoras inmediatas.\n",
    "\n",
    "## Pol√≠tica √ìptima\n",
    "\n",
    "Una pol√≠tica heur√≠stica razonable ser√≠a:\n",
    "```\n",
    "œÄ(s):\n",
    "    if quality_score < 50 and prompt_structure == 'basic':\n",
    "        return 'add_examples'\n",
    "    \n",
    "    elif quality_score < 70 and prompt_structure == 'with_examples':\n",
    "        return 'restructure'\n",
    "    \n",
    "    elif quality_score < 85 and prompt_structure == 'structured':\n",
    "        return 'add_cot' or 'adjust_temperature'\n",
    "    \n",
    "    elif quality_score >= 85:\n",
    "        return 'deploy'\n",
    "    \n",
    "    elif iteration_count >= 5:\n",
    "        return 'test_edge_cases' (validar antes de decidir)\n",
    "```\n",
    "\n",
    "## Ejemplo de Trayectoria √ìptima\n",
    "```\n",
    "Estado inicial: s0 = (25, 0, 'basic')\n",
    "\n",
    "s0 = (25, 0, 'basic')\n",
    "    ‚Üì [add_examples]\n",
    "s1 = (45, 1, 'with_examples')     R = +15\n",
    "    ‚Üì [restructure]\n",
    "s2 = (65, 2, 'structured')        R = +15\n",
    "    ‚Üì [add_constraints]\n",
    "s3 = (75, 3, 'structured')        R = +5\n",
    "    ‚Üì [add_cot]\n",
    "s4 = (88, 4, 'advanced')          R = +108\n",
    "    ‚Üì [deploy]\n",
    "s5 = TERMINAL                     R = 0\n",
    "\n",
    "Recompensa total: 15 + 15 + 5 + 108 = 143\n",
    "Pasos: 4 iteraciones\n",
    "```\n",
    "\n",
    "## Ejemplo de Trayectoria Sub√≥ptima\n",
    "```\n",
    "Estado inicial: s0 = (25, 0, 'basic')\n",
    "\n",
    "s0 = (25, 0, 'basic')\n",
    "    ‚Üì [adjust_temperature]\n",
    "s1 = (30, 1, 'basic')             R = 0\n",
    "    ‚Üì [simplify] (inv√°lido, asume que se permite)\n",
    "s2 = (28, 2, 'basic')             R = -7\n",
    "    ‚Üì [add_examples]\n",
    "s3 = (48, 3, 'with_examples')     R = +15\n",
    "    ‚Üì [adjust_temperature]\n",
    "s4 = (52, 4, 'with_examples')     R = -1\n",
    "    ‚Üì [restructure]\n",
    "s5 = (70, 5, 'structured')        R = +13\n",
    "    ‚Üì [add_cot]\n",
    "s6 = (75, 6, 'structured')        R = -48 (exceso)\n",
    "\n",
    "Recompensa total: 0 - 7 + 15 - 1 + 13 - 48 = -28\n",
    "Pasos: 6 iteraciones (l√≠mite alcanzado sin producci√≥n)\n",
    "```\n",
    "\n",
    "## Matriz de Transici√≥n Simplificada\n",
    "\n",
    "Para el estado `s = (45, 1, 'with_examples')`:\n",
    "\n",
    "| Acci√≥n | s' (quality, iter, struct) | P(s'\\|s,a) | R(s,a,s') |\n",
    "|--------|---------------------------|------------|-----------|\n",
    "| add_examples | (50, 2, 'with_examples') | 0.70 | 0 |\n",
    "| restructure | (65, 2, 'structured') | 0.50 | +15 |\n",
    "| restructure | (55, 2, 'structured') | 0.30 | +5 |\n",
    "| restructure | (40, 2, 'structured') | 0.20 | -15 |\n",
    "| adjust_temperature | (52, 2, 'with_examples') | 0.60 | +2 |\n",
    "| add_constraints | (58, 2, 'with_examples') | 0.65 | +8 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
