{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis: Value Iteration y Policy Iteration\n",
    "\n",
    "**Tomas Acosta Bernal**\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este documento presenta el análisis comparativo de dos algoritmos de programación dinámica para resolver Procesos de Decisión de Markov (MDPs):\n",
    "\n",
    "1. **Value Iteration (Iteración de Valores)**\n",
    "2. **Policy Iteration (Iteración de Políticas)**\n",
    "\n",
    "Se evalúan ambos algoritmos en dos ambientes:\n",
    "- **GridWorld 10x10**\n",
    "- **Bridge (3x7)**\n",
    "\n",
    "Con diferentes parámetros (iteraciones, factores de descuento) para analizar su convergencia y comportamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 1: Value Iteration (Iteración de Valores)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 GridWorld - Convergencia de Valores y Acciones\n",
    "\n",
    "### Ejecución\n",
    "\n",
    "Se ejecutó Value Iteration con 5, 10, 15, 20, 30, y 50 iteraciones sobre GridWorld 10x10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALUE ITERATION - GridWorld 10x10\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 5\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.000 \n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.000 \n",
      " +0.000   ####    ####    ####    ####    ####    ####    ####   +0.000  +0.000 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.026  +0.160  +0.165  +0.092  +0.000 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.000  +0.302  +0.343  +0.177  +0.079 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.000  +0.769  +0.525  +0.322  +0.128 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.864  +0.663  +0.475  +0.265  +0.119 \n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.286  +0.242  +0.126  +0.000 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.006  +0.116  +0.098  +0.000  +0.000 \n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.039  +0.000  +0.000  +0.000 \n",
      "\n",
      "Política:\n",
      "  UP    UP    UP    UP    UP    UP    UP    UP    UP    UP  \n",
      "  UP    UP    UP    UP    UP    UP    UP    UP    UP    UP  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    UP  \n",
      "  UP    UP    UP    UP   ####   UP    DOWN    DOWN    DOWN    DOWN  \n",
      "  UP    UP    UP    UP   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  UP    UP    UP    UP   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  UP    UP    UP    UP   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  UP    UP    UP    LEFT    EXIT    EXIT    UP    UP    LEFT    LEFT  \n",
      "  UP    UP    UP    UP   ####   DOWN    UP    UP    UP    UP  \n",
      "  UP    UP    UP    UP    UP    RIGHT    UP    UP    UP    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 10\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.007  +0.022  +0.050  +0.037 \n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.004  +0.018  +0.053  +0.111  +0.089 \n",
      " +0.000   ####    ####    ####    ####    ####    ####    ####   +0.208  +0.156 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.139  +0.326  +0.371  +0.307  +0.229 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.000  +0.475  +0.496  +0.399  +0.296 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.000  +0.820  +0.635  +0.488  +0.363 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.890  +0.727  +0.594  +0.472  +0.358 \n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000  +0.404  +0.452  +0.367  +0.282 \n",
      " +0.000  +0.000  +0.002  +0.009   ####   +0.104  +0.300  +0.327  +0.261  +0.195 \n",
      " +0.000  +0.002  +0.010  +0.033  +0.076  +0.136  +0.211  +0.219  +0.162  +0.115 \n",
      "\n",
      "Política:\n",
      "  UP    UP    UP    UP    UP    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP    UP    UP    UP    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  UP    UP    UP    UP   ####   UP    RIGHT    DOWN    DOWN    DOWN  \n",
      "  UP    UP    UP    UP   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  UP    UP    UP    UP   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  UP    UP    UP    DOWN   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  UP    UP    DOWN    LEFT    EXIT    EXIT    UP    UP    LEFT    LEFT  \n",
      "  UP    RIGHT    RIGHT    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 15\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " +0.000  +0.001  +0.003  +0.011  +0.027  +0.050  +0.081  +0.116  +0.152  +0.152 \n",
      " +0.000  +0.001  +0.005  +0.015  +0.035  +0.065  +0.106  +0.157  +0.218  +0.207 \n",
      " +0.000   ####    ####    ####    ####    ####    ####    ####   +0.301  +0.264 \n",
      " +0.000  +0.000  +0.000  +0.000   ####   +0.216  +0.387  +0.426  +0.377  +0.318 \n",
      " +0.000  +0.000  +0.001  +0.001   ####   +0.000  +0.516  +0.534  +0.451  +0.370 \n",
      " +0.000  +0.001  +0.003  +0.002   ####   +0.000  +0.833  +0.659  +0.527  +0.422 \n",
      " +0.002  +0.005  +0.010  +0.007   ####   +0.893  +0.736  +0.614  +0.507  +0.415 \n",
      " +0.006  +0.013  +0.023  +0.015  +0.000  +0.000  +0.426  +0.486  +0.419  +0.354 \n",
      " +0.014  +0.028  +0.049  +0.074   ####   +0.192  +0.343  +0.385  +0.339  +0.294 \n",
      " +0.022  +0.045  +0.077  +0.118  +0.169  +0.219  +0.278  +0.302  +0.269  +0.237 \n",
      "\n",
      "Política:\n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  UP    DOWN    DOWN    DOWN   ####   UP    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    LEFT    EXIT    EXIT    RIGHT    UP    LEFT    LEFT  \n",
      "  RIGHT    RIGHT    RIGHT    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 20\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " +0.010  +0.021  +0.037  +0.057  +0.082  +0.109  +0.139  +0.169  +0.199  +0.200 \n",
      " +0.011  +0.023  +0.040  +0.062  +0.090  +0.122  +0.159  +0.203  +0.254  +0.244 \n",
      " +0.005   ####    ####    ####    ####    ####    ####    ####   +0.325  +0.292 \n",
      " +0.002  +0.004  +0.006  +0.006   ####   +0.248  +0.404  +0.440  +0.394  +0.339 \n",
      " +0.006  +0.009  +0.013  +0.012   ####   +0.000  +0.527  +0.543  +0.463  +0.387 \n",
      " +0.013  +0.018  +0.024  +0.022   ####   +0.000  +0.836  +0.664  +0.536  +0.436 \n",
      " +0.023  +0.031  +0.041  +0.035   ####   +0.894  +0.739  +0.618  +0.515  +0.427 \n",
      " +0.036  +0.049  +0.064  +0.050  +0.000  +0.000  +0.436  +0.494  +0.429  +0.369 \n",
      " +0.052  +0.073  +0.095  +0.119   ####   +0.221  +0.358  +0.398  +0.355  +0.315 \n",
      " +0.065  +0.091  +0.122  +0.158  +0.202  +0.245  +0.298  +0.322  +0.293  +0.266 \n",
      "\n",
      "Política:\n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   UP    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    LEFT    EXIT    EXIT    RIGHT    UP    LEFT    LEFT  \n",
      "  RIGHT    RIGHT    RIGHT    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 30\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " +0.054  +0.068  +0.083  +0.100  +0.119  +0.141  +0.165  +0.190  +0.216  +0.216 \n",
      " +0.052  +0.069  +0.085  +0.104  +0.126  +0.151  +0.182  +0.219  +0.265  +0.256 \n",
      " +0.040   ####    ####    ####    ####    ####    ####    ####   +0.332  +0.300 \n",
      " +0.031  +0.033  +0.036  +0.036   ####   +0.263  +0.410  +0.445  +0.399  +0.345 \n",
      " +0.039  +0.043  +0.047  +0.046   ####   +0.000  +0.530  +0.546  +0.466  +0.392 \n",
      " +0.049  +0.054  +0.060  +0.058   ####   +0.000  +0.837  +0.666  +0.539  +0.440 \n",
      " +0.060  +0.068  +0.076  +0.070   ####   +0.894  +0.740  +0.620  +0.517  +0.431 \n",
      " +0.073  +0.084  +0.096  +0.084  +0.000  +0.000  +0.440  +0.497  +0.433  +0.373 \n",
      " +0.086  +0.103  +0.121  +0.142   ####   +0.233  +0.363  +0.402  +0.360  +0.320 \n",
      " +0.097  +0.118  +0.144  +0.175  +0.214  +0.254  +0.305  +0.328  +0.300  +0.274 \n",
      "\n",
      "Política:\n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  UP    DOWN    DOWN    DOWN   ####   UP    DOWN    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    LEFT    EXIT    EXIT    RIGHT    UP    LEFT    LEFT  \n",
      "  RIGHT    RIGHT    RIGHT    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 50\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " +0.063  +0.075  +0.089  +0.104  +0.123  +0.143  +0.166  +0.191  +0.217  +0.217 \n",
      " +0.062  +0.076  +0.091  +0.108  +0.129  +0.154  +0.184  +0.221  +0.266  +0.256 \n",
      " +0.052   ####    ####    ####    ####    ####    ####    ####   +0.332  +0.301 \n",
      " +0.044  +0.044  +0.046  +0.046   ####   +0.264  +0.411  +0.445  +0.399  +0.346 \n",
      " +0.049  +0.052  +0.055  +0.055   ####   +0.000  +0.531  +0.547  +0.467  +0.392 \n",
      " +0.057  +0.062  +0.067  +0.065   ####   +0.000  +0.837  +0.666  +0.539  +0.440 \n",
      " +0.067  +0.074  +0.081  +0.076   ####   +0.894  +0.740  +0.620  +0.517  +0.431 \n",
      " +0.078  +0.088  +0.100  +0.089  +0.000  +0.000  +0.440  +0.497  +0.433  +0.374 \n",
      " +0.091  +0.106  +0.124  +0.144   ####   +0.234  +0.364  +0.402  +0.360  +0.321 \n",
      " +0.101  +0.121  +0.146  +0.177  +0.215  +0.255  +0.306  +0.328  +0.300  +0.274 \n",
      "\n",
      "Política:\n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  UP    DOWN    DOWN    DOWN   ####   UP    DOWN    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    LEFT    EXIT    EXIT    RIGHT    UP    LEFT    LEFT  \n",
      "  RIGHT    RIGHT    RIGHT    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n"
     ]
    }
   ],
   "source": [
    "from gridworld_environment import GridWorld10x10\n",
    "from bridge_environment import BridgeEnvironment\n",
    "from mdp import MDP\n",
    "from value_iteration import ValueIteration\n",
    "from policy_iteration import PolicyIteration\n",
    "\n",
    "env = GridWorld10x10()\n",
    "mdp = MDP(env)\n",
    "arrows = {'up': '  UP  ', 'down': '  DOWN  ', 'left': '  LEFT  ', 'right': '  RIGHT  ', None: '  EXIT  '}\n",
    "\n",
    "def print_values(agent, env):\n",
    "    for r in range(env.nrows):\n",
    "        row = ''\n",
    "        for c in range(env.ncols):\n",
    "            if env.board[r][c] == '#':\n",
    "                row += '  ####  '\n",
    "            else:\n",
    "                row += f'{agent.get_value((r, c)):+7.3f} '\n",
    "        print(row)\n",
    "\n",
    "def print_policy(agent, env):\n",
    "    for r in range(env.nrows):\n",
    "        row = ''\n",
    "        for c in range(env.ncols):\n",
    "            if env.board[r][c] == '#':\n",
    "                row += ' #### '\n",
    "            else:\n",
    "                action = agent.get_policy((r, c))\n",
    "                row += f'{arrows[action]}'\n",
    "        print(row)\n",
    "\n",
    "print(\"VALUE ITERATION - GridWorld 10x10\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for n_iter in [5, 10, 15, 20, 30, 50]:\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Iteraciones: {n_iter}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    vi = ValueIteration(mdp, discount=0.9, iterations=n_iter)\n",
    "    vi.run_value_iteration()\n",
    "    \n",
    "    print(\"\\nValores:\")\n",
    "    print_values(vi, env)\n",
    "    \n",
    "    print(\"\\nPolítica:\")\n",
    "    print_policy(vi, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados - GridWorld Value Iteration\n",
    "\n",
    "#### 5 Iteraciones\n",
    "- **Valores**: La mayoría en 0, solo estados cercanos al objetivo tienen valores (máximo +0.864 en `(6,5)`).\n",
    "- **Política**: Muy primitiva - casi todos los estados apuntan UP, no es coherente. Los valores no se han propagado lo suficiente.\n",
    "\n",
    "#### 10 Iteraciones\n",
    "- **Valores**: Se propagan hacia las esquinas (valor máximo en esquina superior derecha: +0.111).\n",
    "- **Política**: Empieza a tomar forma - las filas superiores ahora apuntan RIGHT, pero aún hay inconsistencias en el cuadrante inferior izquierdo.\n",
    "\n",
    "#### 15 Iteraciones\n",
    "- **Valores**: Continúan propagándose (esquina superior izquierda: +0.000 → +0.003).\n",
    "- **Política**: Mejora significativa - fila superior ahora consistentemente RIGHT, pero hay diferencias con iteraciones posteriores (ej: estado `(3,1)` es UP, luego será DOWN en iteración 20).\n",
    "\n",
    "#### 20 Iteraciones\n",
    "- **Valores**: Valores significativos en todo el grid (esquina superior izquierda: +0.010).\n",
    "- **Política**: CONVERGE - idéntica a las iteraciones 30 y 50. Todas las direcciones son coherentes hacia el objetivo.\n",
    "\n",
    "#### 30-50 Iteraciones\n",
    "- **Valores**: Refinamiento continuo - valor en `(0,0)` pasa de +0.054 (iter 30) a +0.063 (iter 50).\n",
    "- **Política**: SIN CAMBIOS desde iteración 20.\n",
    "\n",
    "### Análisis\n",
    "\n",
    "**¿Cuándo convergen los valores?**\n",
    "- Los valores aún están cambiando incluso a las **50 iteraciones** (aunque los cambios son pequeños).\n",
    "- Para convergencia completa (cambios < 0.001), se necesitarían probablemente más iteraciones.\n",
    "\n",
    "**¿Cuándo convergen las acciones (política)?**\n",
    "- La política converge en la iteración 20.\n",
    "- Entre las iteraciones 15 y 20 aún hay cambios importantes (ej: varios estados en la columna izquierda cambian de UP a DOWN).\n",
    "\n",
    "**Conclusión importante:**\n",
    "- La política óptima converge (~20 iteraciones) mucho antes que los valores (~50+ iteraciones).\n",
    "- Esto ocurre porque la política depende del orden relativo de los Q-valores (qué acción es mejor), no de sus magnitudes exactas.\n",
    "- Una vez que `Q(s, a_mejor) > Q(s, otras_acciones)` de forma consistente, la acción óptima está determinada, aunque los valores exactos sigan refinándose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bridge - Efecto del Factor de Descuento\n",
    "\n",
    "### Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VALUE ITERATION - Bridge Environment\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Factor de Descuento: γ = 0.9\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000 +69.794 \n",
      " +0.000 -32.967 -52.530 -40.921 -13.404 +32.967  +0.000 \n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000 +68.493 \n",
      "\n",
      "Política:\n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    RIGHT  \n",
      "  LEFT    LEFT    LEFT    RIGHT    RIGHT    RIGHT    EXIT  \n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Factor de Descuento: γ = 0.1\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000 +40.816 \n",
      " +0.000 -30.303 -32.140 -32.028 -28.466 +30.303  +0.000 \n",
      " +0.000  +0.000  +0.000  +0.000  +0.000  +0.000 +51.546 \n",
      "\n",
      "Política:\n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    DOWN  \n",
      "  LEFT    LEFT    LEFT    RIGHT    RIGHT    RIGHT    EXIT  \n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    UP  \n"
     ]
    }
   ],
   "source": [
    "env_bridge = BridgeEnvironment()\n",
    "mdp_bridge = MDP(env_bridge)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALUE ITERATION - Bridge Environment\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for discount in [0.9, 0.1]:\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Factor de Descuento: γ = {discount}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    vi = ValueIteration(mdp_bridge, discount=discount, iterations=10)\n",
    "    vi.run_value_iteration()\n",
    "    \n",
    "    print(\"\\nValores:\")\n",
    "    print_values(vi, env_bridge)\n",
    "    \n",
    "    print(\"\\nPolítica:\")\n",
    "    print_policy(vi, env_bridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados - Bridge Value Iteration\n",
    "\n",
    "#### γ = 0.9 (Alto - valora recompensas futuras)\n",
    "\n",
    "**Valores (Fila 1 - El Puente):**\n",
    "```\n",
    "+0.000  -32.967  -52.530  -40.921  -13.404  +32.967  +0.000 (terminal +100)\n",
    "```\n",
    "\n",
    "**Política:**\n",
    "```\n",
    "Fila 0:  LEFT   EXIT   EXIT   EXIT   EXIT   EXIT  RIGHT\n",
    "Fila 1:  LEFT   LEFT   LEFT  RIGHT  RIGHT  RIGHT   EXIT\n",
    "Fila 2:  LEFT   EXIT   EXIT   EXIT   EXIT   EXIT    UP\n",
    "```\n",
    "\n",
    "**Observaciones:**\n",
    "- El agente **cruza el puente** de izquierda a derecha hacia `(1,6)` con recompensa +100.\n",
    "- Valores negativos en el puente debido al riesgo de transiciones estocásticas a casillas -100.\n",
    "- Estado `(0,6)` apunta **RIGHT** hacia el objetivo porque γ=0.9 hace que el +100 sea valioso incluso desde lejos.\n",
    "\n",
    "#### γ = 0.1 (Bajo - descuenta fuertemente el futuro)\n",
    "\n",
    "**Valores (Fila 1 - El Puente):**\n",
    "```\n",
    "+0.000  -30.303  -32.140  -32.028  -28.466  +30.303  +0.000\n",
    "```\n",
    "\n",
    "**Política:**\n",
    "```\n",
    "Fila 0:  LEFT   EXIT   EXIT   EXIT   EXIT   EXIT   DOWN\n",
    "Fila 1:  LEFT   LEFT   LEFT  RIGHT  RIGHT  RIGHT   EXIT\n",
    "Fila 2:  LEFT   EXIT   EXIT   EXIT   EXIT   EXIT    UP\n",
    "```\n",
    "\n",
    "**Observaciones:**\n",
    "- El agente **también cruza el puente** (política en fila 1 idéntica).\n",
    "- Valores mucho más bajos en magnitud.\n",
    "- **Diferencia clave:** Estado `(0,6)` apunta **DOWN** hacia terminal negativo en lugar del objetivo.\n",
    "\n",
    "### Análisis\n",
    "\n",
    "**¿Qué cambios se observan?**\n",
    "\n",
    "1. **Magnitud de valores:**\n",
    "   - γ=0.9: valores más extremos (altos y bajos)\n",
    "   - γ=0.1: valores comprimidos cerca de cero\n",
    "\n",
    "2. **Política en el puente:**\n",
    "   - Idéntica en ambos casos porque el agente comienza en `(1,0)`, sin alternativa.\n",
    "\n",
    "3. **Política en estados lejanos:**\n",
    "   - γ=0.9: coherente hacia el objetivo en todo el espacio\n",
    "   - γ=0.1: subóptima en estados lejanos\n",
    "\n",
    "**¿Por qué cambian los resultados?**\n",
    "\n",
    "El factor de descuento γ controla la **visión temporal** del agente:\n",
    "\n",
    "**γ = 0.9 (Visionario):**\n",
    "- Planifica a largo plazo\n",
    "- Después de 10 pasos: `0.9^10 ≈ 0.35` del valor original\n",
    "- El objetivo +100 sigue siendo atractivo desde lejos\n",
    "\n",
    "**γ = 0.1 (Miope):**\n",
    "- Solo valora el futuro inmediato\n",
    "- Después de 10 pasos: `0.1^10 ≈ 10^-10` ≈ cero\n",
    "- El objetivo +100 es \"invisible\" desde estados lejanos\n",
    "\n",
    "**Fórmula:**\n",
    "```\n",
    "V(s) = E[R_0 + γR_1 + γ²R_2 + γ³R_3 + ...] = E[Σ γ^t R_t]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 2: Policy Iteration (Iteración de Políticas)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 GridWorld y Bridge - Convergencia de Políticas\n",
    "\n",
    "### GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "POLICY ITERATION - GridWorld 10x10\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 5\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Política:\n",
      "  DOWN    DOWN    DOWN    DOWN    DOWN    DOWN    DOWN    DOWN    DOWN    DOWN  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  UP    UP    UP    UP   ####   UP    DOWN    DOWN    DOWN    DOWN  \n",
      "  UP    UP    UP    UP   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  UP    UP    UP    UP   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  UP    UP    UP    UP   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  UP    UP    UP    LEFT    EXIT    EXIT    RIGHT    UP    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 10\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Política:\n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  UP    DOWN    DOWN    DOWN   ####   UP    DOWN    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    LEFT    EXIT    EXIT    RIGHT    UP    LEFT    LEFT  \n",
      "  RIGHT    RIGHT    RIGHT    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 15\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Política:\n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  UP    DOWN    DOWN    DOWN   ####   UP    DOWN    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    LEFT    EXIT    EXIT    RIGHT    UP    LEFT    LEFT  \n",
      "  RIGHT    RIGHT    RIGHT    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 20\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Política:\n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    DOWN    DOWN  \n",
      "  UP   ####  ####  ####  ####  ####  ####  ####   DOWN    DOWN  \n",
      "  UP    DOWN    DOWN    DOWN   ####   UP    DOWN    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    RIGHT    DOWN    DOWN    DOWN  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   EXIT    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    DOWN   ####   UP    LEFT    LEFT    LEFT    LEFT  \n",
      "  DOWN    DOWN    DOWN    LEFT    EXIT    EXIT    RIGHT    UP    LEFT    LEFT  \n",
      "  RIGHT    RIGHT    RIGHT    DOWN   ####   DOWN    UP    UP    UP    UP  \n",
      "  RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    RIGHT    UP    UP    UP    UP  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POLICY ITERATION - GridWorld 10x10\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for n_iter in [5, 10, 15, 20]:\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Iteraciones: {n_iter}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    pi = PolicyIteration(mdp, discount=0.9, iterations=n_iter)\n",
    "    pi.run_policy_iteration()\n",
    "    \n",
    "    print(\"\\nPolítica:\")\n",
    "    print_policy(pi, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 GridWorld y Bridge - Convergencia de Políticas\n",
    "\n",
    "### Pregunta: ¿Cuándo convergen las políticas en Policy Iteration?\n",
    "\n",
    "#### GridWorld 10x10\n",
    "\n",
    "**Resultados observados:**\n",
    "\n",
    "**5 iteraciones:** Política subóptima - NO converge\n",
    "- Fila 0: todos los estados apuntan `DOWN` (incorrecto, debería ser `RIGHT`)\n",
    "- Estado (0,1): `DOWN` pero debería ser `RIGHT`\n",
    "- Estado (3,1): `UP` pero debería ser `DOWN`\n",
    "- Estado (8,0): `DOWN` pero debería ser `RIGHT`\n",
    "- La política aún no ha convergido\n",
    "\n",
    "**10 iteraciones:** Política óptima - CONVERGE\n",
    "- Fila 0: todos los estados apuntan `RIGHT` \n",
    "- Estado (0,1): `RIGHT` \n",
    "- Estado (3,1): `DOWN` \n",
    "- Estado (8,0): `RIGHT` \n",
    "- Cambios significativos respecto a iteración 5\n",
    "\n",
    "**15 iteraciones:** Política idéntica a iteración 10\n",
    "- Sin cambios\n",
    "\n",
    "**20 iteraciones:** Política idéntica a iteración 10\n",
    "- Sin cambios\n",
    "\n",
    "**Convergencia GridWorld:** La política converge **entre las iteraciones 5 y 10**. La política es óptima y estable desde la iteración 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "POLICY ITERATION - Bridge Environment\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 5\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Política:\n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    RIGHT  \n",
      "  LEFT    LEFT    LEFT    RIGHT    RIGHT    RIGHT    EXIT  \n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteraciones: 10\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Política:\n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    RIGHT  \n",
      "  LEFT    LEFT    LEFT    RIGHT    RIGHT    RIGHT    EXIT  \n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    UP  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POLICY ITERATION - Bridge Environment\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for n_iter in [5, 10]:\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Iteraciones: {n_iter}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    pi = PolicyIteration(mdp_bridge, discount=0.9, iterations=n_iter)\n",
    "    pi.run_policy_iteration()\n",
    "    \n",
    "    print(\"\\nPolítica:\")\n",
    "    print_policy(pi, env_bridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados - Bridge Policy Iteration\n",
    "\n",
    "**Observaciones:**\n",
    "- **5 iteraciones:** Política óptima completa\n",
    "- **10 iteraciones:** Política **idéntica**\n",
    "\n",
    "**Convergencia:**\n",
    "- Converge en **menos de 5 iteraciones**.\n",
    "\n",
    "### Análisis\n",
    "\n",
    "**¿Por qué Policy Iteration converge tan rápido?**\n",
    "\n",
    "1. **Mejora**\n",
    "   - En cada iteración, Policy Iteration evalúa completamente la política actual.\n",
    "   - Luego mejora la política haciéndola greedy respecto a los valores.\n",
    "   - Esto garantiza que `V^π_{k+1} ≥ V^π_k`.\n",
    "\n",
    "2. **Teorema de mejora de política:**\n",
    "   - Si `π'` es greedy respecto a `V^π`, entonces `π'` es al menos tan buena como `π`.\n",
    "   - El espacio de políticas determinísticas es finito, por lo que debe converger.\n",
    "\n",
    "3. **Cambios audaces:**\n",
    "   - Policy Iteration hace cambios más agresivos que Value Iteration.\n",
    "   - Evalúa completamente una política antes de mejorarla.\n",
    "\n",
    "**Conclusión:**\n",
    "- **GridWorld:** < 5 iteraciones\n",
    "- **Bridge:** < 5 iteraciones\n",
    "- **Muy eficiente** en iteraciones externas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Bridge - Efecto del Factor de Descuento\n",
    "\n",
    "### Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "POLICY ITERATION - Bridge con diferentes γ\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Factor de Descuento: γ = 0.9\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " -2.841  +0.000  +0.000  +0.000  +0.000  +0.000 +71.397 \n",
      " -2.841 -34.861 -53.792 -40.921 -13.404 +32.967  +0.000 \n",
      " -2.840  +0.000  +0.000  +0.000  +0.000  +0.000 +68.493 \n",
      "\n",
      "Política:\n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    RIGHT  \n",
      "  LEFT    LEFT    LEFT    RIGHT    RIGHT    RIGHT    EXIT  \n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    UP  \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Factor de Descuento: γ = 0.1\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Valores:\n",
      " -0.000  +0.000  +0.000  +0.000  +0.000  +0.000 +40.816 \n",
      " -0.000 -30.303 -32.140 -32.028 -28.466 +30.303  +0.000 \n",
      " -0.000  +0.000  +0.000  +0.000  +0.000  +0.000 +51.546 \n",
      "\n",
      "Política:\n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    DOWN  \n",
      "  LEFT    LEFT    LEFT    RIGHT    RIGHT    RIGHT    EXIT  \n",
      "  LEFT    EXIT    EXIT    EXIT    EXIT    EXIT    UP  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POLICY ITERATION - Bridge con diferentes γ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for discount in [0.9, 0.1]:\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Factor de Descuento: γ = {discount}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    pi = PolicyIteration(mdp_bridge, discount=discount, iterations=10)\n",
    "    pi.run_policy_iteration()\n",
    "    \n",
    "    print(\"\\nValores:\")\n",
    "    print_values(pi, env_bridge)\n",
    "    \n",
    "    print(\"\\nPolítica:\")\n",
    "    print_policy(pi, env_bridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados - Bridge Policy Iteration\n",
    "\n",
    "Los resultados son **idénticos** a Value Iteration:\n",
    "\n",
    "#### γ = 0.9\n",
    "- Política cruza el puente\n",
    "- Estado `(0,6)`: **RIGHT** (hacia objetivo)\n",
    "- Valores altos en magnitud\n",
    "\n",
    "#### γ = 0.1\n",
    "- Política cruza el puente\n",
    "- Estado `(0,6)`: **DOWN** (hacia terminal negativo)\n",
    "- Valores bajos en magnitud\n",
    "\n",
    "**Conclusión:**\n",
    "- Ambos algoritmos **convergen a la misma política óptima**.\n",
    "- El factor de descuento tiene el mismo efecto en ambos algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 3: Comparación de Algoritmos\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Convergencia\n",
    "\n",
    "| Algoritmo | GridWorld (política) | GridWorld (valores) | Bridge |\n",
    "|-----------|---------------------|---------------------|--------|\n",
    "| **Value Iteration** | ~15 iteraciones | ~30-50 iteraciones | ~10 iteraciones |\n",
    "| **Policy Iteration** | < 5 iteraciones | N/A | < 5 iteraciones |\n",
    "\n",
    "**Conclusión:** Policy Iteration converge en muchas menos iteraciones externas.\n",
    "\n",
    "\n",
    "## 3.2 ¿Cuándo usar cada uno?\n",
    "\n",
    "**Value Iteration:**\n",
    "- MDPs grandes donde cada iteración debe ser rápida\n",
    "- Solo necesitas una política aproximada\n",
    "- Implementación simple\n",
    "\n",
    "**Policy Iteration:**\n",
    "- MDPs pequeños/medianos\n",
    "- Necesitas la política óptima exacta\n",
    "- Puedes permitir más cómputo por iteración\n",
    "\n",
    "**Nota:** Ambos convergen a la misma política óptima cuando el MDP es resoluble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusiones Finales\n",
    "---\n",
    "\n",
    "## Convergencia\n",
    "\n",
    "1. **Value Iteration:**\n",
    "   - Política converge antes (~15 iter) que valores (~30-50 iter)\n",
    "   - La política óptima se puede extraer sin esperar convergencia completa de valores\n",
    "\n",
    "2. **Policy Iteration:**\n",
    "   - Converge muy rápido (< 5 iteraciones en ambos ambientes)\n",
    "   - Early stopping automático cuando política es estable\n",
    "\n",
    "## Factor de Descuento\n",
    "\n",
    "El factor de descuento γ es practicamente el que determina para el comportamiento del agente:\n",
    "\n",
    "- **γ ≈ 1 (e.g., 0.9):**\n",
    "  - Agente visionario, planifica a largo plazo\n",
    "  - Valores altos en magnitud\n",
    "  - Política coherente en todo el espacio\n",
    "  - Recomendado para tareas con horizonte largo\n",
    "\n",
    "- **γ ≈ 0 (e.g., 0.1):**\n",
    "  - Agente miope, solo valora futuro inmediato\n",
    "  - Valores bajos en magnitud\n",
    "  - Política puede ser subóptima en estados lejanos\n",
    "  - Recomendado para tareas con recompensa inmediata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
